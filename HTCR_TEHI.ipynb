{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Tamil Character Recognition (HTCR) \n",
    "## uTHCD and HPLabs Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import losses\n",
    "from keras.utils import np_utils\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import ndimage\n",
    "from keras.layers.core import Dropout, Flatten, Dense, Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adamax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os,cv2,imutils\n",
    "import numpy as np\n",
    "# Get the training classes names and store them in a list\n",
    "train_path = \"D:/Datasets/train\"\n",
    "training_names = os.listdir(train_path)\n",
    "# Get all the path to the images and save them in a list\n",
    "# image_paths and the corresponding label in image_classes\n",
    "image_paths = []\n",
    "image_classes = []\n",
    "class_id = 0\n",
    "for training_name in training_names:\n",
    "    dir = os.path.join(train_path, training_name)\n",
    "    class_path = imutils.imlist(dir)\n",
    "    image_paths+=class_path\n",
    "    image_classes+=[class_id]*len(class_path)\n",
    "    print(len(class_path))\n",
    "    class_id+=1\n",
    "\n",
    "print(len(image_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "width=64\n",
    "height=64\n",
    "\n",
    "img_data_list=[]\n",
    "for image_path in image_paths:\n",
    "    print(image_path)\n",
    "    im = cv2.imread(image_path)\n",
    "    im=cv2.resize(im,(width,height),interpolation = cv2.INTER_AREA)\n",
    "    img_data_list.append(im)\n",
    "    \n",
    "img_data = np.array(img_data_list)\n",
    "img_data = img_data.astype('float32')\n",
    "img_data /= 255\n",
    "print (img_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data= np.expand_dims(img_data, axis=3) \n",
    "print (img_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=156\n",
    "labels=image_classes\n",
    "Y = np_utils.to_categorical(labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), padding=\"same\", name=\"Conv1\",input_shape=(width,height,3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "        \n",
    "# first CONV => RELU => POOL\n",
    "model.add(Conv2D(16, (3, 3), padding=\"same\",name=\"Conv2\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\",name=\"Conv3\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\",name=\"Conv4\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\",name=\"Conv5\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# set of FC => Dropout layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(200))\n",
    "\n",
    "# softmax classifier\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "        \n",
    "amax=Adamax(lr=0.001,beta_1=0.9,beta_2=0.999)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= amax,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the dataset\n",
    "x,y = shuffle(img_data,Y, random_state=2)\n",
    "# Split the dataset ( Training and validation split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "epochs=5\n",
    "t=time.time()\n",
    "print('Training sample size', len(X_train))\n",
    "print('Validation sample size', len(X_test))\n",
    "hist= model.fit(X_train, y_train, batch_size=64, epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "print('Training time: %s' % (time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "# plt.legend(['train','val'],loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-white\")\n",
    "plt.figure(1,figsize=(8,6.5))\n",
    "epochs=50\n",
    "plt.plot(np.arange(0,epochs),hist.history['loss'],label=\"train_loss\")\n",
    "plt.plot(np.arange(0,epochs),hist.history['val_loss'],label=\"val_loss\")\n",
    "plt.plot(np.arange(0,epochs),hist.history['accuracy'],label=\"train_accuracy\")\n",
    "plt.plot(np.arange(0,epochs),hist.history['val_accuracy'],label=\"val_accuracy\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Training loss and accuracy on HPLabs dataset\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# Prediction of single test image\n",
    "image = cv2.imread('image/Aa.tiff')\n",
    "image=cv2.resize(image,(width,height),interpolation=cv2.INTER_AREA)\n",
    "#image = image.astype('float32') / 255\n",
    "#image= np.expand_dims(image, axis=0)\n",
    "print(image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "image= np.expand_dims(image, axis=3)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = model.predict(image) \n",
    "i = np.argmax(preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,cv2,imutils\n",
    "import numpy as np\n",
    "# Get the training classes names and store them in a list\n",
    "test_path = r\"D:\\Datasets\\test\"\n",
    "test_names = os.listdir(test_path)\n",
    "# Get all the path to the images and save them in a list\n",
    "# image_paths and the corresponding label in image_classes\n",
    "image_paths = []\n",
    "t_classes = []\n",
    "class_id = 0\n",
    "for test_name in test_names:\n",
    "    dir = os.path.join(test_path, test_name)\n",
    "    class_path = imutils.imlist(dir)\n",
    "    image_paths+=class_path\n",
    "    t_classes+=[class_id]*len(class_path)\n",
    "    print(len(class_path))\n",
    "    class_id+=1\n",
    "\n",
    "print(len(t_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width=64\n",
    "height=64\n",
    "\n",
    "test_data_list=[]\n",
    "for image_path in image_paths:\n",
    "    print(image_path)\n",
    "    im = cv2.imread(image_path,0)\n",
    "    im=cv2.resize(im,(width,height),interpolation = cv2.INTER_AREA)\n",
    "    test_data_list.append(im)\n",
    "    \n",
    "test_data = np.array(test_data_list)\n",
    "test_data = test_data.astype('float32')\n",
    "test_data /= 255\n",
    "print (test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#if dimension error occurs , change the axis value 4\n",
    "test_data= np.expand_dims(test_data, axis=3) \n",
    "print (test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=156\n",
    "labels=t_classes\n",
    "print(labels[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "map_data_L=np.loadtxt(\"hp_uthcd.txt\",dtype=int ,usecols=(1,))\n",
    "print(labels[100:110])\n",
    "print(labels[730:735])\n",
    "print(len(labels))\n",
    "## This code is for interchanging changing the labels of hplabs to uthcd labels ( folder wise)\n",
    "for i,num in enumerate(labels):\n",
    "    labels[i]=map_data_L[num]\n",
    "print(len(map_data_L))\n",
    "print(labels[100:110])\n",
    "print(labels[730:735])\n",
    "#test_labels=np.array(test_data_L)\n",
    "#print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels=np.array(labels)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data, batch_size=64)\n",
    "preds=np.argmax(predictions,axis=1)\n",
    "# Create a boolean array whether each image is correctly classified.\n",
    "correct = (test_labels == preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test=28080\n",
    "# Calculate the number of correctly classified images.\n",
    "# When summing a boolean array, False means 0 and True means 1.\n",
    "correct_sum = correct.sum()\n",
    "# Classification accuracy is the number of correctly classified\n",
    "# images divided by the total number of images in the test-set.\n",
    "acc = float(correct_sum) / num_test\n",
    "# Print the accuracy.\n",
    "msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "print(msg.format(acc, correct_sum, num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(classification_report(preds, test_labels))\n",
    "cm=confusion_matrix(preds, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(5,figsize=(20,20))\n",
    "#confmat=np.random.rand(90,90)\n",
    "ticks=np.linspace(0, 156,num=156)\n",
    "plt.imshow(cm, interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xticks(ticks,fontsize=6)\n",
    "plt.yticks(ticks,fontsize=6)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing of HPLABs dataset with uTHCD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training classes names and store them in a list\n",
    "test_path = \"../Datasets/test/\"   # HPLAbs test data \n",
    "test_names = os.listdir(test_path)\n",
    "# Get all the path to the images and save them in a list\n",
    "# image_paths and the corresponding label in image_classes\n",
    "test_data_list=[]\n",
    "for test_name in test_names:\n",
    "    print(test_name)\n",
    "    dir = os.path.join(test_path, test_name)\n",
    "    print(dir)\n",
    "    test_image = cv2.imread(dir,0)\n",
    "    test_img=cv2.resize(test_image,(width,height),interpolation=cv2.INTER_AREA)\n",
    "    test_data_list.append(test_img)\n",
    "\n",
    "test_data=np.array(test_data_list)\n",
    "test_data = test_data.astype('float32')\n",
    "test_data /= 255\n",
    "print (test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data= np.expand_dims(test_data, axis=3) \n",
    "print (test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_L=np.loadtxt(\"ground.txt\",dtype=int ,usecols=(1,))\n",
    "print(test_data_L[100:110])\n",
    "print(test_data_L[730:735])\n",
    "print(len(test_data_L))\n",
    "#count1=0,count2=0\n",
    "## This code is for interchanging 11 and 155 class as I hve named it differently\n",
    "for i,num in enumerate(test_data_L):\n",
    "    if num==11:\n",
    "        test_data_L[i]=155\n",
    "        #count1=count1+1\n",
    "        continue\n",
    "    if num==155:\n",
    "        test_data_L[i]=11\n",
    "        #count2=count2+1\n",
    "#print \"number of 11: \",count1\n",
    "#print \"number of 155: \",count2\n",
    "print(len(test_data_L))\n",
    "print(test_data_L[100:110])\n",
    "print(test_data_L[730:735])\n",
    "test_labels=np.array(test_data_L)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thisis for mapping hplabs test data labels with same labels as uTHCD \n",
    "test_data_L=np.loadtxt(\"ground.txt\",dtype=int ,usecols=(1,))\n",
    "map_data_L=np.loadtxt(\"hp_uthcd.txt\",dtype=int ,usecols=(1,))\n",
    "print(test_data_L[100:110])\n",
    "print(test_data_L[730:735])\n",
    "print(len(test_data_L))\n",
    "#count1=0,count2=0\n",
    "## This code is for interchanging 11 and 155 class as I hve named it differently\n",
    "for i,num in enumerate(test_data_L):\n",
    "    test_data_L[i]=map_data_L[num]\n",
    "print(len(map_data_L))\n",
    "print(map_data_L[100:110])\n",
    "test_labels=np.array(test_data_L)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data, batch_size=64)\n",
    "preds=np.argmax(predictions,axis=1)\n",
    "# Create a boolean array whether each image is correctly classified.\n",
    "correct = (test_labels == preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test=26926\n",
    "# Calculate the number of correctly classified images.\n",
    "# When summing a boolean array, False means 0 and True means 1.\n",
    "correct_sum = correct.sum()\n",
    "# Classification accuracy is the number of correctly classified\n",
    "# images divided by the total number of images in the test-set.\n",
    "acc = float(correct_sum) / num_test\n",
    "# Print the accuracy.\n",
    "msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "print(msg.format(acc, correct_sum, num_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-CPU",
   "language": "python",
   "name": "htcr_tcpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
